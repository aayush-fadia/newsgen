{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import string\n",
    "real_data = pd.read_csv('True.csv')\n",
    "fake_data = pd.read_csv('Fake.csv')\n",
    "real_articles = real_data[\"text\"]\n",
    "fake_articles = fake_data[\"text\"]\n",
    "translator = {}\n",
    "for punct in string.punctuation:\n",
    "    translator[punct] = \" \"\n",
    "translator[\".\"] = \" . \"\n",
    "translator[\",\"] = \" , \"\n",
    "trans = str.maketrans(translator)\n",
    "OUTFILE = \"articles.txt\"\n",
    "def preprocess_article(article_raw):\n",
    "    article = ' '.join(article_raw.split(\"-\")[1:]).strip().lower()\n",
    "    article = re.sub('u\\.s\\.', \"USA\", article)\n",
    "    article = article.translate(trans)\n",
    "    article = re.sub('\\d{4}', \" <YEAR> \", article)\n",
    "    article = re.sub('\\d+', \" # \", article)\n",
    "    article = re.sub('#.+#', \"#\", article)\n",
    "    article = article.encode(\"ascii\", \"ignore\").decode()\n",
    "    article = article.replace(\"\\n\", \" \")\n",
    "    article = re.sub(\"# jpexyr\", \" \", article)\n",
    "    article += \" <EOA>\"\n",
    "    article = \"<SOA> \"+article\n",
    "    article = re.sub(\" +\", \" \", article)\n",
    "    article = article.strip()\n",
    "    return article\n",
    "with open(OUTFILE, \"w\") as f:\n",
    "    for article_raw in real_articles:\n",
    "        article = preprocess_article(article_raw)\n",
    "        if len(article.split(\" \"))>20:\n",
    "            f.write(article)\n",
    "            f.write(\"\\n\")\n",
    "    for article_raw in fake_articles:\n",
    "        article = preprocess_article(article_raw)\n",
    "        if len(article.split(\" \"))>20:\n",
    "            f.write(article)\n",
    "            f.write(\"\\n\")\n",
    "all_words = []\n",
    "with open(OUTFILE, \"r\") as f:\n",
    "    for line in f.readlines():\n",
    "        line_words = line.strip().split(\" \")\n",
    "        all_words += line_words\n",
    "unique_words = list(set(all_words))\n",
    "print(\"Total Words: {}\".format(len(all_words)))\n",
    "print(\"Unique Words: {}\".format(len(unique_words)))\n",
    "print(\"Average count of each word: {}\".format(len(all_words)/len(unique_words)))\n",
    "from collections import Counter\n",
    "c = Counter(all_words)\n",
    "neg_index = len(c)-VOCAB_SIZE+1\n",
    "disqualify_words = c.most_common()[-neg_index:-1]\n",
    "word2token = dict()\n",
    "token2word = dict()\n",
    "word2token[\"<pad>\"] = 0\n",
    "token2word[0] = \"<pad>\"\n",
    "i = 1\n",
    "for word in c:\n",
    "    word2token[word] = i\n",
    "    token2word[i] = word\n",
    "    i+=1\n",
    "import pickle\n",
    "pickle.dump(word2token, open(\"word2token.pkl\", \"wb\"))\n",
    "pickle.dump(token2word, open(\"token2word.pkl\", \"wb\"))\n",
    "tokenized_articles = []\n",
    "with open(OUTFILE, 'r') as f:\n",
    "    for line in f.readlines():\n",
    "        words = line.strip().split(\" \")\n",
    "        tokenized_article = []\n",
    "        for word in words:\n",
    "            try:\n",
    "                tokenized_article.append(word2token[word])\n",
    "            except KeyError:\n",
    "                print(word)\n",
    "                continue\n",
    "        tokenized_articles.append(tokenized_article)\n",
    "print(\"Total Number of Articles: {}\".format(len(tokenized_articles)))\n",
    "pickle.dump(tokenized_articles, open(\"tokenized_articles.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "word2token = pickle.load(open(\"word2token.pkl\", \"rb\"))\n",
    "token2word = pickle.load(open(\"token2word.pkl\", \"rb\"))\n",
    "tokenized_articles = pickle.load(open(\"tokenized_articles.pkl\", 'rb'))\n",
    "VOCAB_SIZE = 10000\n",
    "def get_next_example():\n",
    "    for article in tokenized_articles:\n",
    "        if(any([token>=VOCAB_SIZE for token in article])):\n",
    "            continue\n",
    "        article = [0]*32+article\n",
    "        article_length = len(article)\n",
    "        for word_number in range(article_length-32):\n",
    "            yield article[word_number: word_number+32], article[word_number+32]\n",
    "import tensorflow\n",
    "physical_devices = tensorflow.config.list_physical_devices('GPU')\n",
    "tensorflow.config.experimental.set_memory_growth(physical_devices[0], enable=True)\n",
    "import tensorflow as tf\n",
    "ds = tf.data.Dataset.from_generator(get_next_example, output_types=(tf.int64, tf.int64), output_shapes = ((32, ), ()))\n",
    "ds = ds.batch(512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sample(gen_start):\n",
    "    gen_tokens = [word2token[word] for word in gen_start.strip().split(\" \")]\n",
    "    gen_tokens = [0]*32+gen_tokens\n",
    "    for i in range(300):\n",
    "        next_token = np.argmax(model.predict(np.asarray([gen_tokens[-33:]]))[0])\n",
    "        gen_tokens = gen_tokens+[next_token]\n",
    "        if next_token == word2token[\"<EOA>\"]:\n",
    "            break\n",
    "    return \" \".join([token2word[token] for token in gen_tokens[32:]])\n",
    "generate_sample(\"<SOA> the trump administration\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from tensorflow.keras.callbacks import Callback\n",
    "from tensorflow.summary import create_file_writer, scalar, text\n",
    "import tensorflow.keras.backend as TF\n",
    "from tensorflow.keras.models import Sequential, save_model\n",
    "class MyCallback(Callback):\n",
    "    def __init__(self, run_base_dir):\n",
    "        self.save_counter = 0\n",
    "        self.least_loss = -1\n",
    "        self.base_dir = run_base_dir\n",
    "        os.makedirs(self.base_dir, exist_ok=True)\n",
    "        os.makedirs(os.path.join(self.base_dir, \"models\"), exist_ok=True)\n",
    "        os.makedirs(os.path.join(self.base_dir, \"logs\"), exist_ok=True)\n",
    "        self.summary_writer = create_file_writer(os.path.join(self.base_dir, \"logs\"))\n",
    "        self.summary_writer.set_as_default()\n",
    "        self.iters_since_last_model_save = 0\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        iter_no = TF.get_value(self.model.optimizer.iterations)\n",
    "        loss = logs['loss']\n",
    "        scalar(\"loss\", data=loss, step = iter_no)\n",
    "        text(\"sample\", generate_sample(\"<SOA> the trump administration\"), step=iter_no)\n",
    "        self.iters_since_last_model_save += 1\n",
    "        if self.least_loss<0 or loss<self.least_loss:\n",
    "            self.least_loss = loss\n",
    "            print(\"Loss decreased in iter {}\".format(iter_no))\n",
    "            if self.iters_since_last_model_save>0:\n",
    "                print(\"Saving model at iteration {} with loss {}\".format(iter_no, loss))\n",
    "                save_model(self.model, os.path.join(self.base_dir, \"models\", \"model-{0:.4f}.h5\".format(loss)))\n",
    "                self.iters_since_last_model_save = 0\n",
    "#     def apply_lr(self):\n",
    "#         TF.set_value(self.model.optimizer.lr, K.get_value(0.0001))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
    "from tensorflow.keras.models import Sequential, save_model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import numpy as np\n",
    "model = Sequential()\n",
    "model.add(Embedding(VOCAB_SIZE, 300))\n",
    "model.add(LSTM(256, return_sequences=True))\n",
    "model.add(LSTM(128))\n",
    "model.add(Dense(VOCAB_SIZE, activation='softmax'))\n",
    "model.summary()\n",
    "model.compile(Adam(0.001), loss='sparse_categorical_crossentropy')\n",
    "for x, y in ds.take(1):\n",
    "    print(model.predict(x).shape)\n",
    "    print(y)\n",
    "model.fit(ds, epochs=100, callbacks=[MyCallback(\"second\")], verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_30\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_30 (Embedding)     (None, None, 300)         3000000   \n",
      "_________________________________________________________________\n",
      "lstm_44 (LSTM)               (None, None, 256)         570368    \n",
      "_________________________________________________________________\n",
      "lstm_45 (LSTM)               (None, 128)               197120    \n",
      "_________________________________________________________________\n",
      "dense_18 (Dense)             (None, 10000)             1290000   \n",
      "=================================================================\n",
      "Total params: 5,057,488\n",
      "Trainable params: 5,057,488\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "model = load_model(\"second/models/model-1.3227.h5\")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<SOA> the trump administration is ready in principle to resume population americans rather than a total of hate on twitter , when the president shared her aim of raising awareness of issues long as it was related to their capital . if a similar group report for trump s visit to pyongyang , and they should be tackled head from the united kingdom . we are doing a lot of people , including teachers , and cannot go through . the sheer length of the newly discovered most recently was told the office of management and does not think the united states has become no , he said . the memo was sent to that flynn was a former army of the south china sea , and i am not going to be released by the right , in a preamble that we recognize their closest could do something do be done , especially . . . it is expected to endorse trump campaign colluded with russia , the white house said in a statement . <EOA>'"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_sample(\"<SOA> the trump administration\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 300)\n",
      "70717\n"
     ]
    }
   ],
   "source": [
    "import io\n",
    "\n",
    "RUN_NAME = \"second\"\n",
    "out_v = io.open(os.path.join(RUN_NAME, 'vecs.tsv'), 'w', encoding='utf-8')\n",
    "out_m = io.open(os.path.join(RUN_NAME, 'meta.tsv'), 'w', encoding='utf-8')\n",
    "e = model.layers[0]\n",
    "weights = e.get_weights()[0]\n",
    "print(weights.shape)\n",
    "print(len(token2word))\n",
    "for x in range(VOCAB_SIZE):\n",
    "    word = token2word[x]\n",
    "    if not word in ['', \" \", '\\t', \"\\n\", \"  \"]:\n",
    "        if(len(word)>2):\n",
    "            vec = weights[x]\n",
    "            out_m.write(token2word[x]+\"\\n\")\n",
    "            out_v.write('\\t'.join([str(x) for x in vec])+\"\\n\")\n",
    "out_v.close()\n",
    "out_m.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.1 64-bit ('dl_superenv': virtualenv)",
   "language": "python",
   "name": "python38164bitdlsuperenvvirtualenvcf9ff5001a704b55a5e75bb19dfc1b11"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
